{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n## NOTE: Even though we use the PyTorch module, we import it with the name 'torch', which was the original name.\n\nimport torch # torch provides basic functions, from setting a random seed (for reproducability) to creating tensors.\nimport torch.nn as nn # torch.nn allows us to create a neural network.\nimport torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\nfrom torch.optim import Adam # optim contains many optimizers. Here, we're using Adam, which is similar to stochastic gradient descent but not as stochastic so it is faster.\n\nimport pytorch_lightning as L # lightning has tons of cool tools that make neural networks easier\nfrom torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n\nimport matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\nimport seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-23T10:09:31.074281Z","iopub.execute_input":"2024-10-23T10:09:31.075685Z","iopub.status.idle":"2024-10-23T10:09:39.321724Z","shell.execute_reply.started":"2024-10-23T10:09:31.075612Z","shell.execute_reply":"2024-10-23T10:09:39.320352Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# input data\ntoken_to_id = {'merhaba':0,\n               'iyi':1,\n               'gunler':2,\n               'nasilsiniz':3,\n               'iyiyim':4,\n               '<EOS>':5,\n            }\n\nid_to_token=dict(map(reversed, token_to_id.items()))\n            ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:39.323876Z","iopub.execute_input":"2024-10-23T10:09:39.324490Z","iopub.status.idle":"2024-10-23T10:09:39.330667Z","shell.execute_reply.started":"2024-10-23T10:09:39.324444Z","shell.execute_reply":"2024-10-23T10:09:39.329270Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"inputs = torch.tensor([[token_to_id[\"merhaba\"],\n                      token_to_id[\"iyi\"],\n                      token_to_id[\"gunler\"],\n                      token_to_id[\"nasilsiniz\"],\n                      token_to_id[\"<EOS>\"],\n                      token_to_id[\"iyiyim\"]],\n                       \n                      [token_to_id[\"iyi\"],\n                      token_to_id[\"gunler\"],\n                      token_to_id[\"merhaba\"],\n                      token_to_id[\"nasilsiniz\"],\n                      token_to_id[\"<EOS>\"],\n                      token_to_id[\"iyiyim\"]]])\n\nlabels = torch.tensor([[token_to_id[\"iyi\"],\n                      token_to_id[\"gunler\"],\n                      token_to_id[\"nasilsiniz\"],\n                      token_to_id[\"<EOS>\"],\n                      token_to_id[\"iyiyim\"],\n                      token_to_id[\"<EOS>\"]],\n                       \n                      [token_to_id[\"gunler\"],\n                      token_to_id[\"merhaba\"],\n                      token_to_id[\"nasilsiniz\"],\n                      token_to_id[\"<EOS>\"],\n                      token_to_id[\"iyiyim\"],\n                      token_to_id[\"<EOS>\"]]])\n\ndataset= TensorDataset(inputs, labels)\ndataloader=DataLoader(dataset) ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:39.332491Z","iopub.execute_input":"2024-10-23T10:09:39.332925Z","iopub.status.idle":"2024-10-23T10:09:39.353627Z","shell.execute_reply.started":"2024-10-23T10:09:39.332882Z","shell.execute_reply":"2024-10-23T10:09:39.352232Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class PositionEncoding(nn.Module):\n    \n    def __init__(self, d_model=2, max_len=6):\n        \n        super().__init__()\n        \n        self.pe = torch.zeros(max_len, d_model) # creating matrix of zeros\n        \n        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n        \n        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n        \n        div_term = 1/torch.tensor(10000)**(embedding_index/d_model)\n        \n        self.pe[:, 0:2] = torch.sin(position*div_term)\n        self.pe[:, 1:2] = torch.cos(position*div_term)\n        \n    def forward(self, word_embeddings):\n        \n        return word_embeddings + self.pe[:word_embeddings.size(0), :]\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:39.355056Z","iopub.execute_input":"2024-10-23T10:09:39.355507Z","iopub.status.idle":"2024-10-23T10:09:39.366742Z","shell.execute_reply.started":"2024-10-23T10:09:39.355464Z","shell.execute_reply":"2024-10-23T10:09:39.365414Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Attention(nn.Module):\n    \n    def __init__(self, d_model=2):\n        \n        super().__init__()\n        \n        self.W_q=nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        self.W_k=nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        self.W_v=nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        \n        self.row_dim=0\n        self.col_dim=1\n        \n    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n        \n        q = self.W_q(encodings_for_q)\n        k = self.W_k(encodings_for_k)\n        v = self.W_v(encodings_for_v)\n        \n        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n        \n        scaled_sims = sims/torch.tensor(k.size(self.col_dim)**0.5)\n        \n        if mask is not None:\n            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n            \n        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n        \n        attention_scores = torch.matmul(attention_percents, v)\n        \n        return attention_scores\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:39.370486Z","iopub.execute_input":"2024-10-23T10:09:39.371096Z","iopub.status.idle":"2024-10-23T10:09:39.384002Z","shell.execute_reply.started":"2024-10-23T10:09:39.371035Z","shell.execute_reply":"2024-10-23T10:09:39.382570Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class DecoderOnlyTransformer(L.LightningModule):\n    \n    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n        \n        super().__init__()\n        \n        self.we = nn.Embedding(num_embeddings=num_tokens,\n                               embedding_dim=d_model)\n        self.pe = PositionEncoding(d_model=d_model, \n                                   max_len=max_len)\n        self.self_attention = Attention(d_model=d_model)\n        \n        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n        \n        self.loss = nn.CrossEntropyLoss()\n        \n    def forward (self, token_ids):\n        \n        word_embeddings = self.we(token_ids)\n        position_encoded = self.pe(word_embeddings)\n\n        mask = torch.tril(torch.ones((token_ids.size(dim=0),token_ids.size(dim=0))))\n        mask = mask == 0\n        \n        self_attention_values = self.self_attention(position_encoded,\n                                                    position_encoded,\n                                                    position_encoded,\n                                                    mask=mask)\n        residual_connection_values = position_encoded + self_attention_values\n        \n        fc_layer_output = self.fc_layer(residual_connection_values)\n        \n        return fc_layer_output\n    \n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=0.1)\n    \n    def training_step(self, batch, batchidx):\n        \n        input_tokens, labels = batch\n        output = self.forward(input_tokens[0])\n        loss = self.loss(output, labels[0])\n        \n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:39.386093Z","iopub.execute_input":"2024-10-23T10:09:39.386578Z","iopub.status.idle":"2024-10-23T10:09:39.401665Z","shell.execute_reply.started":"2024-10-23T10:09:39.386535Z","shell.execute_reply":"2024-10-23T10:09:39.400186Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)\n\nmodel_input = torch.tensor([token_to_id[\"merhaba\"],\n                            token_to_id[\"iyi\"],\n                            token_to_id[\"gunler\"],\n                            token_to_id[\"nasilsiniz\"],\n                            token_to_id[\"<EOS>\"]])\n\ninput_length = model_input.size(dim=0)\n\npredictions = model(model_input)\npredicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\npredicted_ids = predicted_id\n\nmax_length=6\n\nfor i in range(input_length, max_length):\n    if (predicted_id == token_to_id[\"<EOS>\"]):\n        break\n        \n    model_input = torch.cat((model_input, predicted_id), dim=0)\n    \n    predictions = model(model_input)\n    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n    predicted_ids = torch.cat((predicted_ids, predicted_id), dim=0)\n    \nprint(\"Predicted tokens:\\n\")\nfor id in predicted_ids:\n    print(\"\\t\", id_to_token[id.item()])","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:39.403448Z","iopub.execute_input":"2024-10-23T10:09:39.404421Z","iopub.status.idle":"2024-10-23T10:09:39.579190Z","shell.execute_reply.started":"2024-10-23T10:09:39.404356Z","shell.execute_reply":"2024-10-23T10:09:39.577850Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Predicted tokens:\n\n\t iyi\n\t gunler\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"trainer = L.Trainer(max_epochs=30)\n\ntrainer.fit(model, train_dataloaders=dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:39.580812Z","iopub.execute_input":"2024-10-23T10:09:39.581251Z","iopub.status.idle":"2024-10-23T10:09:56.617647Z","shell.execute_reply.started":"2024-10-23T10:09:39.581181Z","shell.execute_reply":"2024-10-23T10:09:56.616270Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a6bc45a95b4de18de557a2b1bf2daf"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"model_input = torch.tensor([token_to_id[\"merhaba\"],\n                            token_to_id[\"iyi\"],\n                            token_to_id[\"gunler\"],\n                            token_to_id[\"nasilsiniz\"],\n                            token_to_id[\"<EOS>\"]])\n\ninput_length = model_input.size(dim=0)\n\npredictions = model(model_input)\npredicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\npredicted_ids = predicted_id\n\nmax_length=6\n\nfor i in range(input_length, max_length):\n    if (predicted_id == token_to_id[\"<EOS>\"]):\n        break\n        \n    model_input = torch.cat((model_input, predicted_id), dim=0)\n    \n    predictions = model(model_input)\n    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n    predicted_ids = torch.cat((predicted_ids, predicted_id), dim=0)\n    \nprint(\"Predicted tokens:\\n\")\nfor id in predicted_ids:\n    print(\"\\t\", id_to_token[id.item()])","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:09:56.618950Z","iopub.execute_input":"2024-10-23T10:09:56.620008Z","iopub.status.idle":"2024-10-23T10:09:56.638843Z","shell.execute_reply.started":"2024-10-23T10:09:56.619957Z","shell.execute_reply":"2024-10-23T10:09:56.637416Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Predicted tokens:\n\n\t iyiyim\n\t <EOS>\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model_input = torch.tensor([token_to_id[\"iyi\"],\n                            token_to_id[\"gunler\"],\n                            token_to_id[\"<EOS>\"]])\n\ninput_length = model_input.size(dim=0)\n\npredictions = model(model_input)\npredicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\npredicted_ids = predicted_id\n\nmax_length=6\n\nfor i in range(input_length, max_length):\n    if (predicted_id == token_to_id[\"<EOS>\"]):\n        break\n        \n    model_input = torch.cat((model_input, predicted_id), dim=0)\n    \n    predictions = model(model_input)\n    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n    predicted_ids = torch.cat((predicted_ids, predicted_id), dim=0)\n    \nprint(\"Predicted tokens:\\n\")\nfor id in predicted_ids:\n    print(\"\\t\", id_to_token[id.item()])","metadata":{"execution":{"iopub.status.busy":"2024-10-23T10:11:20.162145Z","iopub.execute_input":"2024-10-23T10:11:20.162681Z","iopub.status.idle":"2024-10-23T10:11:20.177815Z","shell.execute_reply.started":"2024-10-23T10:11:20.162638Z","shell.execute_reply":"2024-10-23T10:11:20.176538Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Predicted tokens:\n\n\t nasilsiniz\n\t <EOS>\n","output_type":"stream"}],"execution_count":12}]}